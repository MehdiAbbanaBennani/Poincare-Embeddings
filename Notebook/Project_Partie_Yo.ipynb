{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import linalg\n",
    "import sys\n",
    "from smart_open import smart_open\n",
    "import csv\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PATH_TO_DATA = os.path.join(os.getcwd(),'data')\n",
    "os.listdir(PATH_TO_DATA)\n",
    "wordnet_mammal_file = os.path.join(PATH_TO_DATA, 'wordnet_mammal_hypernyms.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PoincareData():\n",
    "    def __init__(self,fname, nmax, verbose = False, doublon_tol = False):\n",
    "        '''\n",
    "        fname: name of the tsv file \n",
    "        nmax: number of extracted lines in the fname file\n",
    "        '''\n",
    "        l = self.load_file(fname,nmax, verbose)\n",
    "        self.build_vocab(l, verbose, doublon_tol)\n",
    "        \n",
    "    def load_file(self,fname, nmax, verbose):\n",
    "        '''\n",
    "        Parsing of the TSV file\n",
    "        '''\n",
    "        entire = True\n",
    "        with open(fname,'r') as f:\n",
    "            l = []\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= nmax:\n",
    "                    entire = False\n",
    "                    break\n",
    "                line = line.strip().split('\\t')\n",
    "                l.append(tuple(line))\n",
    "        if verbose:\n",
    "            print('Entire Parsing: ' + str(entire))\n",
    "        return l\n",
    "                \n",
    "    def build_vocab(self,loaded_file, verbose, doublon_tol = False):\n",
    "        '''\n",
    "        Given a loaded_file, build the index2word, word2index, the vocab, node relations\n",
    "        param:\n",
    "            - vocab: occurence of the different words (defaultdict)\n",
    "            - index2word: index associated to a word (list)\n",
    "            - all_relations : list of tuples containing the interactions (list)\n",
    "            - word2index: (dict)\n",
    "            - node_relations: mapping from node index to its related node indices\n",
    "        '''\n",
    "        self.vocab = defaultdict(lambda: 0)\n",
    "        self.index2word = [] # position du mot dans la liste est l'index\n",
    "        self.all_relations = []\n",
    "        self.word2index = {}\n",
    "        self.node_relations = defaultdict(set)\n",
    "        doublon = 0\n",
    "        #self.wordfrequency = {}\n",
    "        for relation in loaded_file:\n",
    "            if relation[0] == relation[1]:\n",
    "                doublon +=1\n",
    "            if len(relation) != 2:\n",
    "                raise ValueError('Relation pair \"%s\" should be a pair !' % repr(relation))\n",
    "            if (doublon_tol == True):\n",
    "                for w in relation:\n",
    "                    if w in self.vocab:\n",
    "                        self.vocab[w] +=1\n",
    "                    else:\n",
    "                        # new word detected\n",
    "                        self.word2index[w] = len(self.index2word) # we give the new word its own index\n",
    "                        self.index2word.append(w) # new word in the list\n",
    "                        self.vocab[w] = 1 # new key in the vocab dictionary\n",
    "\n",
    "                node1,node2 = relation\n",
    "                node1_index, node2_index = self.word2index[node1], self.word2index[node2]\n",
    "                self.node_relations[node1_index].add(node2_index)\n",
    "                self.all_relations.append((node1_index, node2_index))\n",
    "            else:\n",
    "                if relation[0] != relation[1]:\n",
    "                    for w in relation:\n",
    "                        if w in self.vocab:\n",
    "                            self.vocab[w] +=1\n",
    "                        else:\n",
    "                            # new word detected\n",
    "                            self.word2index[w] = len(self.index2word) \n",
    "                            self.index2word.append(w) \n",
    "                            self.vocab[w] = 1\n",
    "\n",
    "                    node1,node2 = relation\n",
    "                    node1_index, node2_index = self.word2index[node1], self.word2index[node2]\n",
    "                    self.node_relations[node1_index].add(node2_index)\n",
    "                    self.all_relations.append((node1_index, node2_index))\n",
    "        if verbose:\n",
    "            print('Vocabulary Build !')\n",
    "            print(str(doublon) + ' doublons was found')\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire Parsing: False\n",
      "Vocabulary Build !\n",
      "5 doublons was found\n"
     ]
    }
   ],
   "source": [
    "data = PoincareData(wordnet_mammal_file, 20, verbose= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.all_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire Parsing: False\n",
      "Vocabulary Build !\n",
      "5 doublons was found\n"
     ]
    }
   ],
   "source": [
    "data2 = PoincareData(wordnet_mammal_file, 20, verbose = True, doublon_tol=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data2.all_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kangaroo.n.01', 'marsupial.n.01', 'domestic_goat.n.01', 'even-toed_ungulate.n.01', 'rock_squirrel.n.01', 'ground_squirrel.n.02', 'vizsla.n.01', 'dog.n.01', 'dandie_dinmont.n.01', 'mammal.n.01', 'broodmare.n.01', 'horse.n.01', 'lesser_kudu.n.01', 'placental.n.01', 'water_shrew.n.01', 'insectivore.n.01', 'silky_anteater.n.01', 'giant_kangaroo.n.01', 'metatherian.n.01', 'seattle_slew.n.01', 'thoroughbred.n.02', 'boxer.n.04', 'rabbit.n.01', 'longhorn.n.01', 'bovid.n.01', 'blue_fox.n.01', 'fox.n.01']\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "print(data.index2word)\n",
    "print(len(data.index2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kangaroo.n.01', 'marsupial.n.01', 'domestic_goat.n.01', 'even-toed_ungulate.n.01', 'rock_squirrel.n.01', 'ground_squirrel.n.02', 'vizsla.n.01', 'dog.n.01', 'dandie_dinmont.n.01', 'mammal.n.01', 'broodmare.n.01', 'horse.n.01', 'spotted_skunk.n.01', 'hispid_pocket_mouse.n.01', 'lesser_kudu.n.01', 'placental.n.01', 'water_shrew.n.01', 'insectivore.n.01', 'silky_anteater.n.01', 'giant_kangaroo.n.01', 'metatherian.n.01', 'bronco.n.01', 'pekinese.n.01', 'seattle_slew.n.01', 'thoroughbred.n.02', 'kinkajou.n.01', 'boxer.n.04', 'rabbit.n.01', 'longhorn.n.01', 'bovid.n.01', 'blue_fox.n.01', 'fox.n.01']\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "print(data2.index2word)\n",
    "print(len(data2.index2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kangaroo.n.01',\n",
       " 'marsupial.n.01',\n",
       " 'domestic_goat.n.01',\n",
       " 'even-toed_ungulate.n.01',\n",
       " 'rock_squirrel.n.01',\n",
       " 'ground_squirrel.n.02',\n",
       " 'vizsla.n.01',\n",
       " 'dog.n.01',\n",
       " 'dandie_dinmont.n.01',\n",
       " 'mammal.n.01',\n",
       " 'broodmare.n.01',\n",
       " 'horse.n.01',\n",
       " 'lesser_kudu.n.01',\n",
       " 'placental.n.01',\n",
       " 'water_shrew.n.01',\n",
       " 'insectivore.n.01',\n",
       " 'silky_anteater.n.01',\n",
       " 'giant_kangaroo.n.01',\n",
       " 'metatherian.n.01',\n",
       " 'seattle_slew.n.01',\n",
       " 'thoroughbred.n.02',\n",
       " 'boxer.n.04',\n",
       " 'rabbit.n.01',\n",
       " 'longhorn.n.01',\n",
       " 'bovid.n.01',\n",
       " 'blue_fox.n.01',\n",
       " 'fox.n.01']"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kangaroo.n.01',\n",
       " 'marsupial.n.01',\n",
       " 'domestic_goat.n.01',\n",
       " 'even-toed_ungulate.n.01',\n",
       " 'rock_squirrel.n.01',\n",
       " 'ground_squirrel.n.02',\n",
       " 'vizsla.n.01',\n",
       " 'dog.n.01',\n",
       " 'dandie_dinmont.n.01',\n",
       " 'mammal.n.01',\n",
       " 'broodmare.n.01',\n",
       " 'horse.n.01',\n",
       " 'spotted_skunk.n.01',\n",
       " 'hispid_pocket_mouse.n.01',\n",
       " 'lesser_kudu.n.01',\n",
       " 'placental.n.01',\n",
       " 'water_shrew.n.01',\n",
       " 'insectivore.n.01',\n",
       " 'silky_anteater.n.01',\n",
       " 'giant_kangaroo.n.01',\n",
       " 'metatherian.n.01',\n",
       " 'bronco.n.01',\n",
       " 'pekinese.n.01',\n",
       " 'seattle_slew.n.01',\n",
       " 'thoroughbred.n.02',\n",
       " 'kinkajou.n.01',\n",
       " 'boxer.n.04',\n",
       " 'rabbit.n.01',\n",
       " 'longhorn.n.01',\n",
       " 'bovid.n.01',\n",
       " 'blue_fox.n.01',\n",
       " 'fox.n.01']"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {0: {1},\n",
       "             2: {3},\n",
       "             4: {5},\n",
       "             6: {7},\n",
       "             8: {9},\n",
       "             10: {11},\n",
       "             12: {13},\n",
       "             14: {15},\n",
       "             16: {13},\n",
       "             17: {18},\n",
       "             19: {20},\n",
       "             21: {9},\n",
       "             22: {13},\n",
       "             23: {24},\n",
       "             25: {26}})"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.node_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {0: {1},\n",
       "             2: {3},\n",
       "             4: {5},\n",
       "             6: {7},\n",
       "             8: {9},\n",
       "             10: {11},\n",
       "             12: {12},\n",
       "             13: {13},\n",
       "             14: {15},\n",
       "             16: {17},\n",
       "             18: {15},\n",
       "             19: {20},\n",
       "             21: {21},\n",
       "             22: {22},\n",
       "             23: {24},\n",
       "             25: {25},\n",
       "             26: {9},\n",
       "             27: {15},\n",
       "             28: {29},\n",
       "             30: {31}})"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.node_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'broodmare.n.01': 10,\n",
       " 'dandie_dinmont.n.01': 8,\n",
       " 'dog.n.01': 7,\n",
       " 'domestic_goat.n.01': 2,\n",
       " 'even-toed_ungulate.n.01': 3,\n",
       " 'ground_squirrel.n.02': 5,\n",
       " 'hispid_pocket_mouse.n.01': 13,\n",
       " 'horse.n.01': 11,\n",
       " 'insectivore.n.01': 17,\n",
       " 'kangaroo.n.01': 0,\n",
       " 'lesser_kudu.n.01': 14,\n",
       " 'mammal.n.01': 9,\n",
       " 'marsupial.n.01': 1,\n",
       " 'placental.n.01': 15,\n",
       " 'rock_squirrel.n.01': 4,\n",
       " 'spotted_skunk.n.01': 12,\n",
       " 'vizsla.n.01': 6,\n",
       " 'water_shrew.n.01': 16}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.PoincareData.build_vocab.<locals>.<lambda>>,\n",
       "            {'blue_fox.n.01': 1,\n",
       "             'bovid.n.01': 1,\n",
       "             'boxer.n.04': 1,\n",
       "             'broodmare.n.01': 1,\n",
       "             'dandie_dinmont.n.01': 1,\n",
       "             'dog.n.01': 1,\n",
       "             'domestic_goat.n.01': 1,\n",
       "             'even-toed_ungulate.n.01': 1,\n",
       "             'fox.n.01': 1,\n",
       "             'giant_kangaroo.n.01': 1,\n",
       "             'ground_squirrel.n.02': 1,\n",
       "             'horse.n.01': 1,\n",
       "             'insectivore.n.01': 1,\n",
       "             'kangaroo.n.01': 1,\n",
       "             'lesser_kudu.n.01': 1,\n",
       "             'longhorn.n.01': 1,\n",
       "             'mammal.n.01': 2,\n",
       "             'marsupial.n.01': 1,\n",
       "             'metatherian.n.01': 1,\n",
       "             'placental.n.01': 3,\n",
       "             'rabbit.n.01': 1,\n",
       "             'rock_squirrel.n.01': 1,\n",
       "             'seattle_slew.n.01': 1,\n",
       "             'silky_anteater.n.01': 1,\n",
       "             'thoroughbred.n.02': 1,\n",
       "             'vizsla.n.01': 1,\n",
       "             'water_shrew.n.01': 1})"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class abc():\n",
    "    def __init__(self, fname, nmax=100):\n",
    "        self.load_data(fname, nmax)\n",
    "        self.word2id = {}\n",
    "        for i,word in enumerate(self.word2vec.keys()):\n",
    "            self.word2id[word] = i\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "        self.embeddings = np.array(list(self.word2vec.values()))\n",
    "    \n",
    "    def load_wordvec(self, fname, nmax):\n",
    "        self.data = {}\n",
    "        with io.open(fname, encoding='utf-8') as f:\n",
    "            next(f)\n",
    "            for i, line in enumerate(f):\n",
    "                word, vec = line.split(' ', 1)\n",
    "                self.word2vec[word] = np.fromstring(vec, sep=' ')\n",
    "                if i == (nmax - 1):\n",
    "                    break\n",
    "        print('Loaded %s pretrained word vectors' % (len(self.word2vec)))\n",
    "\n",
    "    def most_similar(self, w, K=5):\n",
    "        res = []\n",
    "        score = np.zeros(len(self.id2word.keys()))\n",
    "        for key,val in self.id2word.items():\n",
    "            score[key] = self.score(w,val)\n",
    "        for i in score.argsort()[::-1][1:(K+1)]:\n",
    "            res.append(w2v.id2word[i])\n",
    "        # K most similar words: self.score  -  np.argsort \n",
    "        return res\n",
    "\n",
    "    def score(self, w1, w2):\n",
    "        vec_1 = self.word2vec[w1]\n",
    "        vec_2 = self.word2vec[w2]\n",
    "        return np.dot(vec_1,vec_2)/(np.linalg.norm(vec_1)*np.linalg.norm(vec_2))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
